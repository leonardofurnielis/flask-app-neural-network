{
 "cells": [
  {
   "source": [
    "# IMDB Sentiment Analyses¶\n",
    "Neste notebook estamos utilizando os dados do Kaggle (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-ovie-reviews).\n",
    "\n",
    "Vamos seguir os seguintes passos:\n",
    "\n",
    "1. Importar o Dataset\n",
    "2. Preparar os Dados para Construir o Modelo\n",
    "3. Criar o Dataset de Teste e Treino\n",
    "4. Treino do Modelo Utilizando Keras\n",
    "5. Salva o modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "source": [
    "# 1 - Importar o Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "df = pd.read_csv('./imdb-dataset.csv', delimiter=',')\n",
    "df = df.head(30000)"
   ]
  },
  {
   "source": [
    "# 2 - Preparar os Dados para Construir o Modelo\n",
    "Agora vamos preparar nossos seguindo os passos:\n",
    "\n",
    "1. Tonkenization         \n",
    "2. Remover stopwords\n",
    "3. Stemming text\n",
    "4. Juntar novamente em uma única frase\n",
    "\n",
    "Como estamos trabalhando com uma entrada de texto, realizamos estas etapas para \"normalizar\" nossa base."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    source = row[0]\n",
    "    tokens = word_tokenize(source)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(row):\n",
    "    source_tokenization = row[2]\n",
    "    stop = [w for w in source_tokenization if not w in stop_words]\n",
    "    return (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_porter(row):\n",
    "    my_list = row[2]\n",
    "    stemmed_list = [porter_stemmer.stem(word) for word in my_list]\n",
    "    return (stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    my_list = row[2]\n",
    "    joined_words = (\" \".join(my_list))\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    print('Tokenization')\n",
    "    df['text1'] = df.apply(identify_tokens, axis=1)\n",
    "    print('Remove stop words')\n",
    "    df['text1'] = df.apply(remove_stops, axis=1)\n",
    "    print('Stemming')\n",
    "    df['text1'] = df.apply(stem_porter, axis=1)\n",
    "    print('Rejoin words')\n",
    "    df['tidy_text'] = df.apply(rejoin_words, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_processing(df)\n",
    "df['tidy_text'] = df['tidy_text'].str.lower()\n",
    "\n",
    "X = df['tidy_text']\n",
    "Y = df['sentiment']"
   ]
  },
  {
   "source": [
    "# 3 - Criar o Dataset de Teste e Treino¶\n",
    "Vamos criar o nosso dataset de teste (30%) e treino (70%) de forma balanceado (Stratified)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=48,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3),\n",
    "                        sublinear_tf=True,\n",
    "                        max_features=10000)"
   ]
  },
  {
   "source": [
    "Os modelos de Machine Learning ou Deep Learning esperam como entrada \"X\" um valor numérico. Como estamos trabalhando com texto iremos realizar o processo de TfIdf para transformar o texto em valores numéricos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_test_tf = vectorizer.transform(X_test)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(list(Y_train))\n",
    "Y_train_le = le.transform(list(Y_train))\n",
    "Y_test_le = le.transform(list(Y_test))\n",
    "\n",
    "num_class = Y.value_counts().shape\n",
    "input_shape = X_train_tf.shape"
   ]
  },
  {
   "source": [
    "# 4 - Treino do Modelo Utilizando Keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train_label_keras = to_categorical(Y_train_le)\n",
    "Y_test_label_keras = to_categorical(Y_test_le)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(2, activation='relu', input_shape=(input_shape[1], )))\n",
    "network.add(layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(5, activation='relu'))\n",
    "network.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(5, activation='sigmoid'))\n",
    "network.add(layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(num_class[0], activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='adamax',\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(X_train_tf.toarray(),\n",
    "            Y_train_label_keras,\n",
    "            verbose=1,\n",
    "            epochs=50,\n",
    "            validation_split=0.3)"
   ]
  },
  {
   "source": [
    "# 5 - Salva o Modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('./model/neural_network.h5')\n",
    "pickle.dump(vectorizer, open('./model/vectorizer.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}